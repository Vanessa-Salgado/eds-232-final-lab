---
title: "Lab9 Kaggle"
author: "Vanessa-Salgado and Rosemary Juarez"
format: html
---

## Preparation

```{r}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```



# Team Women in stem

For our 2024 Kaggle Competition for 232 Machine Learning, we ran three different models and chose the most accurate model to predict predict dissolved inorganic carbon in water samples collected by the California Cooperative Oceanic Fisheries Investigations program (CalCOFI). 

We will run three models, compare, and choose the best performing model. To check our model metrics, we will be using RMSE.

Load libraries
```{r, results='hide'}
library(tidyverse) 
library(tidymodels)
library(tensorflow)
library(keras)
library(rsample)
library(glmnet)
library(corrplot)
library(here)
library(xgboost) #package for boosted trees
library(kableExtra) #for formatting tables in html
```

## Read in Data
```{r, results='hide'}
full_train <- read_csv(here("data", "train.csv")) 
final_test_data <- read_csv(here("data", "test.csv"))

# isolate ID column
id_list <- final_test_data$id

# update final test data for model
final_test_data <- final_test_data %>% 
  
  # rename column to match
  rename(TA1.x = TA1) %>% 
  
  select(-id)
```


## Explore the Data
```{r}
# update training data
full_train <- full_train %>% 
  
  # remove column of NAs and unique identifier (id) column
  select(-c(id, ...13))

# view distribution of DIC
ggplot(full_train) +
  geom_histogram(aes(x = DIC)) +
  theme_minimal()
```

```{r}
# checking for multicollinearity 
cor(full_train) %>%
  corrplot()
```

## Preprocessing

In our preprocessing stage, we split our data, created our recipe, and specified our folds for cross validation.

```{r}
# split data
split <- initial_split(full_train)
train <- training(split)
test <- testing(split)

# specify folds for cross-validation
folds <- vfold_cv(train, v = 10)

# split data into test and train
split <- initial_split(train)

# define a recipe (regression formula)
recip <- recipe(DIC ~., data = train) %>% 
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())
```

# Algorithms 

## Linear Regression
We used linear regression for one of our models. This is one method for modeling regression.
our steps include:
- specify model
- create workflow
- train model
- predict using test data
- compare metrics


```{r}
lm_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# Hold modeling components in a workflow
lm_wf <- workflow() %>% 
  add_recipe(recip) %>% 
  add_model(lm_spec)

# Train the model
lm_wf_fit <- lm_wf %>% 
  fit(data = train)

# Make predictions for the test set
predictions <- lm_wf_fit %>% 
  predict(new_data = test)


# Bind predictions to the test set
lm_results <- test %>% 
  bind_cols(predictions)


# Print the first ten rows of the tibble
lm_results %>% 
  slice_head(n = 10)

# Evaluate performance of linear regression
metrics(data = lm_results,
        truth = DIC,
        estimate = .pred)


new_predict_lm <- predict(object = lm_wf_fit, new_data = test) %>% 
  bind_cols(test) %>% 
  select( DIC = .pred)

```

## Random Forest
Another method for using regression is random forest. Random Forest is known for classification tasks, but can also be used for regression. Our steps include:

- creating random forest model. 
- create workflow
- tune hyperoparameters
- finalize workflow
- train model using training data
- predict using testing data
- check metrics

```{r}

## Random Forest Pre-processing


# random forest model specification
rf_spec <- rand_forest(mtry = tune(), 
                       trees = tune(),
                       min_n = tune()) %>%
  set_engine("ranger") %>% 
  set_mode("regression")

# random forest model workflow
rf_wf <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(recip)


## Random Forest Tuning


# cross validation
rf_tune_res <- rf_wf %>% 
  tune_grid(resamples = folds,
            grid = 7)

# finalize workflow
rf_final <- finalize_workflow(rf_wf, select_best(rf_tune_res, metric = "rmse"))


## Random Forest Final Predictions

# final fit
rf_fit <- fit(rf_final, train)

# final prediction on test data
rf_pred_df <- augment(rf_fit, new_data = test)


```


##  Boosted Regression
Extreme Gradient Boosting is known for its efficiency in regression algorithms. Our steps for creating our model include:
- creating model by tuning our regressions
- create a workflow
- isolate parameters
- create grid for our tuning process
- tune
- finalize workflow
- fit our model to our training data
- predict with testing data
- test model performance


```{r}

## Boosted Regression Pre-processing


# create boosted regression tree model specification
xgb_spec <- boost_tree(learn_rate = tune(),
                       trees = tune(),
                       tree_depth = tune(),
                       min_n = tune(),
                       loss_reduction = tune(),
                       mtry = tune(),
                       sample_size = tune()) %>% 
  set_engine("xgboost", nthread = 2) %>% 
  set_mode("regression")

# create xgb model workflow
xgb_wf <- workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(recip)

# isolate parameters
xgb_param <- xgb_spec %>% extract_parameter_set_dials()


## Boosted Regression Tuning

# cross validation
xgb_tune_res <- xgb_wf %>% 
  tune_grid(resamples = folds,
            grid = grid_latin_hypercube(finalize(xgb_param, x = train),
                size = 7))

# finalize workflow
xgb_final <- finalize_workflow(xgb_wf, select_best(xgb_tune_res, metric = "rmse"))



## Boosted Regression Predictions

# final fit
xgb_fit <- fit(xgb_final, train)

# final prediction on test data
xgb_pred_df <- augment(xgb_fit, new_data = test)
```


## Comparing Performances

We want to compare which model performed best out of the three regression models we created. Our table below will show which model is best. 

```{r}
#lm_performance <- rmse(lm_results, truth = dic, estimate = .pred)
rf_performance <- rmse(rf_pred_df, truth = DIC, estimate = .pred)
xgb_performance <- rmse(xgb_pred_df, truth = DIC, estimate = .pred)

model <- c('Random Forest',' Extreme Gradient Boosting')

performance <- rbind(rf_performance, xgb_performance)

comparison <- cbind(model, performance)

kable(comparison)

```


## Predictions for Submission

```{r}
# create final prediction on test data
xgb_final_pred <- augment(xgb_fit, new_data = final_test_data)
rf_final_pred <- augment(rf_fit, new_data = final_test_data)

# bind back to original data to add id column
xgb_final_pred <- xgb_final_pred %>% 
  mutate(id = id_list) %>% 
  rename(DIC = .pred) %>% 
  select(id, DIC)

rf_final_pred <- rf_final_pred %>% 
  mutate(id = id_list) %>% 
  rename(DIC = .pred) %>% 
  select(id, DIC)

# export final prediction data frames

#write_csv(xgb_final_pred, file = here("data", "womeninstem_xgb_pred.csv"))
#write_csv(rf_final_pred, file = here("data", "womeninstem_rf_pred.csv"))
```





