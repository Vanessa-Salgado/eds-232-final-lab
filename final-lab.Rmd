---
title: "Final Lab Kaggle"
author: "Vanessa Salgado and Rosemary Juarez"
format: html
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

Load Libraries
```{r}
library(tidyverse) 
library(tidymodels)
library(tensorflow)
library(keras)
library(rsample)
library(glmnet)
library(corrplot)
library(here)
library(xgboost) #package for boosted trees
library(kableExtra) #for formatting tables in html
```

# Team Women in stem

For our 2024 Kaggle Competition for 232 Machine Learning, we ran three different models and chose the most accurate model to predict predict dissolved inorganic carbon in water samples collected by the California Cooperative Oceanic Fisheries Investigations program (CalCOFI). 

We will run three models, compare, and choose the best performing model. To check our model metrics, we will be using RMSE.


Read in Data
```{r}
set.seed(245)

sample_submission <- read_csv(here::here("data", "sample_submission.csv"))
full_train <- read_csv(here("data", "train.csv")) 
final_test_data <- read_csv(here("data", "test.csv"))

# outcome variable is DIC
train_data <- full_train %>% 
  janitor::clean_names() %>% 
  select(-c(x13, ta1_x))

# test data
test_data <- final_test_data %>% 
  janitor::clean_names()
```

## Explore the data
```{r}
ggplot(train) +
  geom_histogram(aes(x = dic)) +
  theme_bw()

# checking for multicollinearity 
cor(train) %>%
  corrplot()
```


## Preprocessing

In our preprocessing stage, we split our data, created our recipe, and specified our folds for cross validation.
```{r}
set.seed(123)

# split data
split <- initial_split(train_data)
train <- training(split)
test <- testing(split)

# create recipe
dic_recipe <- recipe(dic ~ ., data = train) %>% 
  step_zv(all_predictors()) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors())


# specify folds for cross-validation
# 5 or 10 
v_folds <- vfold_cv(train, v = 10)

```

# Algorithms 

## Linear Regression
We used linear regression for one of our models. This is one method for modeling regression.
our steps include:
- specify model
- create workflow
- train model
- predict using test data
- compare metrics

```{r}
set.seed(123)

# Create a linear model specification
lm_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# Hold modeling components in a workflow
lm_wf <- workflow() %>% 
  add_recipe(dic_recipe) %>% 
  add_model(lm_spec)

# Train the model
lm_wf_fit <- lm_wf %>% 
  fit(data = train)

# Make predictions for the test set
predictions <- lm_wf_fit %>% 
  predict(new_data = test)


# Bind predictions to the test set
lm_results <- test %>% 
  bind_cols(predictions)


# Print the first ten rows of the tibble
lm_results %>% 
  slice_head(n = 10)

# Evaluate performance of linear regression
metrics(data = lm_results,
        truth = dic,
        estimate = .pred)


new_predict_lm <- predict(object = lm_wf_fit, new_data = test) %>% 
  bind_cols(test) %>% 
  select(id, DIC = .pred)
```


## Random Forest
another method for using regression is random forest. Random Forest is known for classification tasks, but can also be used for regression. Our steps include:

- creating random forest model. 
- create workflow
- tune hyperoparameters
- finalize workflow
- train model using training data
- predict using testing data
- check metrics

```{r}
set.seed(234)

#random forest model
rf_spec <- rand_forest(mtry = tune(), 
                        trees = tune()) %>%
  set_engine("ranger") %>% 
  set_mode("regression") 


# create workflow
rf_workflow <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(dic_recipe)

doParallel::registerDoParallel(cores = 4)

# tune
system.time(
  rf_tune <- rf_workflow %>% 
    tune_grid(
      resamples = v_folds, # add folds
      grid = 5 # number of combos
    )
)



rf_final = finalize_workflow(rf_workflow, 
                             select_best(rf_tune, metric = "rmse"))

rf_fit <- fit(rf_final, train) # fit the model to the training data

train_predict <- predict(object = rf_fit, new_data = train) %>% # predict the training set
  bind_cols(train) # bind training set column to prediction

test_predict <- predict(object = rf_fit, new_data = test) %>% # predict the training set
  bind_cols(test) # bind prediction to testing data column

train_metrics <- train_predict %>%
  metrics(dic, .pred) # get testing data metrics

test_metrics <- test_predict %>%
  metrics(dic, .pred) # get testing data metrics

train_metrics
test_metrics

new_predict_rf <- predict(object = rf_fit, new_data = test) %>% 
  bind_cols(test) %>% 
  select(id, DIC = .pred)

# write.csv(new_predict, "week-10/new_predict.csv", row.names=FALSE)
```



##  Boosted Regression
Extreme Gradient Boosting is known for its efficiency in regression algorithms. Our steps for creating our model include:
- creating model by tuning our regressions
- create a workflow
- isolate parameters
- create grid for our tuning process
- tune
- finalize workflow
- fit our model to our training data
- predict with testing data
- test model performance



```{r}
#model for boosted regression
xgb_model = boost_tree(learn_rate = tune(), 
                       trees = tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")


#workflow
xgb_workflow = workflow() %>% #create workflow
  add_model(xgb_model) %>% #add boosted trees model
  add_recipe(dic_recipe) #add recipe

#isolate parameters
xgb_best_model <- xgb_model %>% 
  extract_parameter_set_dials()

#assigning grid
grid2 <- grid_latin_hypercube(xgb_best_model, size = 10)


# grid tuning and tuning
xgb_cv_tune = xgb_workflow %>%
   tune_grid(resamples = v_folds, grid = grid2 ) #use cross validation to tune learn_rate and trees parameters


#showing metrics of our model
collect_metrics(xgb_cv_tune)


autoplot(xgb_cv_tune) + #plot cv results for parameter tuning
  theme_bw()

 #get metrics for best boosted regression
xgb_best = show_best(xgb_cv_tune, metric = "rmse", n = 1)
xgb_best


xgb_final = finalize_workflow(xgb_workflow, select_best(xgb_cv_tune, metric = "rmse"))

train_fit_xgb = fit(xgb_final, train) #fit the Boosted Trees model to the training set


#testing prediction
predict_xgb = predict(train_fit_xgb, test) %>% #get testing prediction
  bind_cols(test)

new_predic_xgb <- predict_xgb %>% 
  select(id, dic = .pred)
  

#testing performance of my model
rmse_test <- metrics(data = predict_xgb, estimate = .pred, truth = dic)
rmse_test

```

## comparing performances

We want to compare which model performed best out of the three regression models we created. Our table below will show which model is best. 
```{r}
lm_performance <- rmse(lm_results, truth = dic, estimate = .pred)
rf_performance <- rmse(test_predict, truth = dic, estimate = .pred)
xgb_performance <- rmse(predict_xgb, truth = dic, estimate = .pred)

model <- c('Linear Regression', 'Random Forest',' Extreme Gradient Boosting')

performance <- rbind(lm_performance, rf_performance, xgb_performance)

comparison <- cbind(model, performance)

kable(comparison)

```



## Saving predictions for the competition
we save our predictions by writing our final predictions in .CSV format

```{r}

write_csv(new_predict_lm, "women_in_stem_lm.csv")
write_csv(new_predict_rf, "women_in_stem_rf.csv")
write_csv(new_predict_xgb, "women_in_stem_xgb.csv")
```




