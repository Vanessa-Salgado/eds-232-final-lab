---
title: "Final Lab Kaggle"
author: "Vanessa Salgado and Rosemary Juarez"
format: html
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

Load Libraries
```{r}
library(tidyverse) 
library(tidymodels)
library(tensorflow)
library(keras)
library(rsample)
library(glmnet)
library(corrplot)
library(here)
library(xgboost) #package for boosted trees
library(kableExtra) #for formatting tables in html
```

# Team Women in stem

For our 2024 Kaggle Competition for 232 Machine Learning, we ran three different models and chose the most accurate model to predict predict dissolved inorganic carbon in water samples collected by the California Cooperative Oceanic Fisheries Investigations program (CalCOFI). 


Read in Data
```{r}
set.seed(245)

sample_submission <- read_csv(here::here("data", "sample_submission.csv"))
full_train <- read_csv(here("data", "train.csv")) 
final_test_data <- read_csv(here("data", "test.csv"))

# outcome variable is DIC
train <- full_train %>% 
  janitor::clean_names() %>% 
  select(-c(x13, ta1_x))

# test data
test <- final_test_data %>% 
  janitor::clean_names()
```

## Explore the data
```{r}
ggplot(train) +
  geom_histogram(aes(x = dic)) +
  theme_bw()

# checking for multicollinearity 
cor(train) %>%
  corrplot()
```


## Preprocessing
```{r}
set.seed(123)

# split data
split <- initial_split(train)
train <- training(split)
test <- testing(split)

# create recipe
dic_recipe <- recipe(dic ~ ., data = train) %>% 
  step_zv(all_predictors()) %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors())


# specify folds for cross-validation
# 5 or 10 
v_folds <- vfold_cv(train, v = 10)

```

# Algorithms 

## Linear Regression

```{r}
set.seed(123)

# Create a linear model specification
lm_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# Hold modeling components in a workflow
lm_wf <- workflow() %>% 
  add_recipe(dic_recipe) %>% 
  add_model(lm_spec)

# Train the model
lm_wf_fit <- lm_wf %>% 
  fit(data = train)

# Make predictions for the test set
predictions <- lm_wf_fit %>% 
  predict(new_data = test)


# Bind predictions to the test set
lm_results <- test %>% 
  bind_cols(predictions)


# Print the first ten rows of the tibble
lm_results %>% 
  slice_head(n = 10)

# Evaluate performance of linear regression
metrics(data = lm_results,
        truth = dic,
        estimate = .pred)


new_predict_lm <- predict(object = lm_wf_fit, new_data = test) %>% 
  bind_cols(test) %>% 
  select(id, DIC = .pred)
```


## Random Forest
```{r}
set.seed(234)

#random forest model
rf_spec <- rand_forest(mtry = tune(), 
                        trees = tune()) %>%
  set_engine("ranger") %>% 
  set_mode("regression") 


# create workflow
rf_workflow <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(dic_recipe)

doParallel::registerDoParallel(cores = 4)

# tune
system.time(
  rf_tune <- rf_workflow %>% 
    tune_grid(
      resamples = v_folds, # add folds
      grid = 5 # number of combos
    )
)



rf_final = finalize_workflow(rf_workflow, 
                             select_best(rf_tune, metric = "rmse"))

rf_fit <- fit(rf_final, train) # fit the data to the training data

train_predict <- predict(object = rf_fit, new_data = train) %>% # predict the training set
  bind_cols(train) # bind training set column to prediction

test_predict <- predict(object = rf_fit, new_data = test) %>% # predict the training set
  bind_cols(test) # bind prediction to testing data column

train_metrics <- train_predict %>%
  metrics(dic, .pred) # get testing data metrics

test_metrics <- test_predict %>%
  metrics(dic, .pred) # get testing data metrics

train_metrics
test_metrics

new_predict_rf <- predict(object = rf_fit, new_data = test) %>% 
  bind_cols(test) %>% 
  select(id, DIC = .pred)

# write.csv(new_predict, "week-10/new_predict.csv", row.names=FALSE)
```



##  Boosted Regression


```{r}
#model for boosted regression
xgb_model = boost_tree(learn_rate = tune(), 
                       trees = tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")


#workflow
xgb_workflow = workflow() %>% #create workflow
  add_model(xgb_model) %>% #add boosted trees model
  add_recipe(dic_recipe) #add recipe

#isolate parameters
xgb_best_model <- xgb_model %>% 
  extract_parameter_set_dials()


grid2 <- grid_latin_hypercube(xgb_best_model, size = 10)


# grid tuning and tuning
xgb_cv_tune = xgb_workflow %>%
   tune_grid(resamples = v_folds, grid = grid2 ) #use cross validation to tune learn_rate and trees parameters


#showing the 
collect_metrics(xgb_cv_tune)


autoplot(xgb_cv_tune) + #plot cv results for parameter tuning
  theme_bw()

 #get metrics for best boosted regression
xgb_best = show_best(xgb_cv_tune, metric = "rmse", n = 1)
xgb_best


xgb_final = finalize_workflow(xgb_workflow, select_best(xgb_cv_tune, metric = "rmse"))

train_fit_xgb = fit(xgb_final, train) #fit the Boosted Trees model to the training set


#testing prediction
predict_xgb = predict(train_fit_xgb, test) %>% #get testing prediction
  bind_cols(test)

new_predic_xgb <- predict_xgb %>% 
  select(id, dic = .pred)
  

#testing performance of my model
rmse_test <- metrics(data = predict_xgb, estimate = .pred, truth = dic)
rmse_test

```

comparing performances
```{r}
lm_performance <- rmse(lm_results, truth = dic, estimate = .pred)
rf_performance <- rmse(test_predict, truth = dic, estimate = .pred)
xgb_performance <- rmse(predict_xgb, truth = dic, estimate = .pred)

model <- c('Linear Regression', 'Random Forest',' Extreme Gradient Boosting')

performance <- rbind(lm_performance, rf_performance, xgb_performance)

comparison <- cbind(model, performance)

kable(comparison)

```



## Saving predictions for the competition

```{r}

write_csv(new_predict_lm, "women_in_stem_lm.csv")
write_csv(new_predict_rf, "women_in_stem_rf.csv")
write_csv(new_predict_xgb, "women_in_stem_xgb.csv")
```




